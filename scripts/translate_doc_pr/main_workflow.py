"""
Main Entry Point for GitHub Workflow
Orchestrates the entire auto-sync workflow in GitHub Actions environment
"""

import sys
import os
import json
import threading
import tiktoken
from github import Github, Auth

# Conditional import for Gemini
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# Import all modules
from pr_analyzer import analyze_source_changes, get_repo_config, get_target_hierarchy_and_content, parse_pr_url
from file_adder import process_added_files
from file_deleter import process_deleted_files
from file_updater import process_files_in_batches, process_added_sections, process_modified_sections, process_deleted_sections
from toc_processor import process_toc_files
from section_matcher import match_source_diff_to_target

# Configuration from environment variables
SOURCE_PR_URL = os.getenv("SOURCE_PR_URL")
TARGET_PR_URL = os.getenv("TARGET_PR_URL")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
AI_PROVIDER = os.getenv("AI_PROVIDER", "deepseek")
TARGET_REPO_PATH = os.getenv("TARGET_REPO_PATH")

# AI configuration
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_TOKEN")
DEEPSEEK_BASE_URL = "https://api.deepseek.com"
GEMINI_API_KEY = os.getenv("GEMINI_API_TOKEN")
GEMINI_MODEL_NAME = "gemini-2.0-flash"

# Processing limit configuration
MAX_NON_SYSTEM_SECTIONS_FOR_AI = 120
SOURCE_TOKEN_LIMIT = 5000  # Maximum tokens for source new_content before skipping file processing

# AI configuration
AI_MAX_TOKENS = 20000  # Maximum tokens for AI translation requests

# Special file configuration
SPECIAL_FILES = ["TOC.md"]
IGNORE_FILES = ["faq/ddl-faq.md","command-line-flags-for-tidb-configuration.md","pd-configuration-file.md"]

# Repository configuration for workflow
def get_workflow_repo_configs():
    """Get repository configuration based on environment variables"""
    if not SOURCE_PR_URL or not TARGET_PR_URL:
        raise ValueError("SOURCE_PR_URL and TARGET_PR_URL must be set")
    
    # Parse source and target repo info
    source_parts = SOURCE_PR_URL.split('/')
    target_parts = TARGET_PR_URL.split('/')
    
    source_owner, source_repo = source_parts[-4], source_parts[-3]
    target_owner, target_repo = target_parts[-4], target_parts[-3]
    
    source_repo_key = f"{source_owner}/{source_repo}"
    target_repo_key = f"{target_owner}/{target_repo}"
    
    # Determine language direction based on repo names
    if source_repo.endswith('-cn') and not target_repo.endswith('-cn'):
        # Chinese to English
        source_language = "Chinese"
        target_language = "English"
    elif not source_repo.endswith('-cn') and target_repo.endswith('-cn'):
        # English to Chinese
        source_language = "English"
        target_language = "Chinese"
    else:
        # Default fallback
        source_language = "English"
        target_language = "Chinese"
    
    return {
        source_repo_key: {
            "target_repo": target_repo_key,
            "target_local_path": TARGET_REPO_PATH,
            "source_language": source_language,
            "target_language": target_language
        }
    }

# Thread-safe printing function
print_lock = threading.Lock()

def thread_safe_print(*args, **kwargs):
    with print_lock:
        print(*args, **kwargs)

def ensure_temp_output_dir():
    """Ensure the temp_output directory exists"""
    # Get the directory of the current script
    script_dir = os.path.dirname(os.path.abspath(__file__))
    temp_dir = os.path.join(script_dir, "temp_output")
    os.makedirs(temp_dir, exist_ok=True)
    return temp_dir

def clean_temp_output_dir():
    """Clean the temp_output directory at the start of execution"""
    import shutil
    # Get the directory of the current script
    script_dir = os.path.dirname(os.path.abspath(__file__))
    temp_dir = os.path.join(script_dir, "temp_output")
    if os.path.exists(temp_dir):
        if os.path.isdir(temp_dir):
            shutil.rmtree(temp_dir)
            print(f"🧹 Cleaned existing temp_output directory")
        else:
            # Remove file if it exists
            os.remove(temp_dir)
            print(f"🧹 Removed existing temp_output file")
    os.makedirs(temp_dir, exist_ok=True)
    print(f"📁 Created temp_output directory: {temp_dir}")
    return temp_dir

def estimate_tokens(text):
    """Calculate accurate token count using tiktoken (GPT-4/3.5 encoding)"""
    if not text:
        return 0
    try:
        enc = tiktoken.get_encoding("cl100k_base")  # GPT-4/3.5 encoding
        tokens = enc.encode(text)
        return len(tokens)
    except Exception as e:
        # Fallback to character approximation if tiktoken fails
        thread_safe_print(f"   ⚠️  Tiktoken encoding failed: {e}, using character approximation")
        return len(text) // 4

def print_token_estimation(prompt_text, context="AI translation"):
    """Print accurate token consumption for a request"""
    actual_tokens = estimate_tokens(prompt_text)
    char_count = len(prompt_text)
    thread_safe_print(f"   💰 {context}")
    thread_safe_print(f"      📝 Input: {char_count:,} characters")
    thread_safe_print(f"      🔢 Actual tokens: {actual_tokens:,} (using tiktoken cl100k_base)")
    return actual_tokens

class UnifiedAIClient:
    """Unified interface for different AI providers"""
    
    def __init__(self, provider="deepseek"):
        self.provider = provider
        if provider == "deepseek":
            from openai import OpenAI
            self.client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)
            self.model = "deepseek-chat"
        elif provider == "gemini":
            if not GEMINI_AVAILABLE:
                raise ImportError("google-generativeai package not installed. Run: pip install google-generativeai")
            if not GEMINI_API_KEY:
                raise ValueError("GEMINI_API_TOKEN environment variable must be set")
            genai.configure(api_key=GEMINI_API_KEY)
            self.model = GEMINI_MODEL_NAME
        else:
            raise ValueError(f"Unsupported AI provider: {provider}")
    
    def chat_completion(self, messages, temperature=0.1, max_tokens=20000):
        """Unified chat completion interface"""
        if self.provider == "deepseek":
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content.strip()
        elif self.provider == "gemini":
            try:
                # Convert OpenAI-style messages to Gemini format
                prompt = self._convert_messages_to_prompt(messages)
                thread_safe_print(f"   🔄 Calling Gemini API...")
                
                # Use the correct Gemini API call format
                model = genai.GenerativeModel(self.model)
                response = model.generate_content(prompt)
                
                if response and response.text:
                    thread_safe_print(f"   ✅ Gemini response received")
                    return response.text.strip()
                else:
                    thread_safe_print(f"   ⚠️  Gemini response was empty or blocked")
                    return "No response from Gemini"
                    
            except Exception as e:
                thread_safe_print(f"   ❌ Gemini API error: {str(e)}")
                # Fallback: suggest switching to DeepSeek
                thread_safe_print(f"   💡 Consider switching to DeepSeek in main.py: AI_PROVIDER = 'deepseek'")
                raise e
    
    def _convert_messages_to_prompt(self, messages):
        """Convert OpenAI-style messages to a single prompt for Gemini"""
        prompt_parts = []
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            if role == "user":
                prompt_parts.append(content)
            elif role == "system":
                prompt_parts.append(f"System: {content}")
        return "\n\n".join(prompt_parts)

def check_source_token_limit(source_diff_dict_file, token_limit=SOURCE_TOKEN_LIMIT):
    """Check if the total tokens of all new_content in source-diff-dict exceeds the limit"""
    try:
        with open(source_diff_dict_file, 'r', encoding='utf-8') as f:
            source_diff_dict = json.load(f)
        
        total_new_content = ""
        section_count = 0
        
        for key, section_data in source_diff_dict.items():
            if isinstance(section_data, dict):
                new_content = section_data.get('new_content', '')
                if new_content:
                    total_new_content += new_content + "\n"
                    section_count += 1
        
        if not total_new_content.strip():
            thread_safe_print(f"   ⚠️  No new_content found in {source_diff_dict_file}")
            return True, 0, 0  # Allow processing if no content to check
        
        total_tokens = estimate_tokens(total_new_content)
        char_count = len(total_new_content)
        
        thread_safe_print(f"   📊 Source token limit check:")
        thread_safe_print(f"      📝 Total new_content: {char_count:,} characters from {section_count} sections")
        thread_safe_print(f"      🔢 Total tokens: {total_tokens:,}")
        thread_safe_print(f"      🚧 Token limit: {token_limit:,}")
        
        if total_tokens > token_limit:
            thread_safe_print(f"      ❌ Token limit exceeded! ({total_tokens:,} > {token_limit:,})")
            return False, total_tokens, token_limit
        else:
            thread_safe_print(f"      ✅ Within token limit ({total_tokens:,} ≤ {token_limit:,})")
            return True, total_tokens, token_limit
            
    except Exception as e:
        thread_safe_print(f"   ❌ Error checking token limit for {source_diff_dict_file}: {e}")
        return True, 0, 0  # Allow processing on error to avoid blocking

def get_pr_diff(pr_url, github_client):
    """Get the diff content from a GitHub PR (from auto-sync-pr-changes.py)"""
    try:
        from pr_analyzer import parse_pr_url
        owner, repo, pr_number = parse_pr_url(pr_url)
        repository = github_client.get_repo(f"{owner}/{repo}")
        pr = repository.get_pull(pr_number)
        
        # Get files and their patches
        files = pr.get_files()
        diff_content = []
        
        for file in files:
            if file.filename.endswith('.md') and file.patch:
                diff_content.append(f"File: {file.filename}")
                diff_content.append(file.patch)
                diff_content.append("-" * 80)
        
        return "\n".join(diff_content)
        
    except Exception as e:
        thread_safe_print(f"   ❌ Error getting PR diff: {e}")
        return None

def filter_diff_by_operation_type(pr_diff, operation_type, target_sections=None):
    """Filter PR diff to only include changes relevant to specific operation type"""
    
    if not pr_diff:
        return ""
    
    if operation_type == "modified":
        # For modified sections, we want the full diff but focus on changed content
        return pr_diff
    elif operation_type == "added":
        # For added sections, we want to show what was added
        filtered_lines = []
        for line in pr_diff.split('\n'):
            if line.startswith('+') and not line.startswith('+++'):
                filtered_lines.append(line)
            elif line.startswith('@@') or line.startswith('File:'):
                filtered_lines.append(line)
        return '\n'.join(filtered_lines)
    elif operation_type == "deleted":
        # For deleted sections, we want to show what was removed
        filtered_lines = []
        for line in pr_diff.split('\n'):
            if line.startswith('-') and not line.startswith('---'):
                filtered_lines.append(line)
            elif line.startswith('@@') or line.startswith('File:'):
                filtered_lines.append(line)
        return '\n'.join(filtered_lines)
    
    return pr_diff

def filter_diff_for_target_file(pr_diff, target_file, source_diff_dict):
    """Extract file-specific diff from the complete PR diff based on source files that map to the target file"""
    if not pr_diff or not source_diff_dict:
        return pr_diff
    
    # Extract source files that contribute to this target file
    source_files = set()
    for key, section_data in source_diff_dict.items():
        if isinstance(section_data, dict):
            source_file = section_data.get('source_file', '')
            if source_file:
                source_files.add(source_file)
    
    if not source_files:
        print(f"   ⚠️  No source files found in source_diff_dict, using complete PR diff")
        return pr_diff
    
    print(f"   📄 Source files contributing to {target_file}: {list(source_files)}")
    
    # Filter PR diff to only include changes from these source files
    filtered_lines = []
    current_file = None
    include_section = False
    
    for line in pr_diff.split('\n'):
        if line.startswith('File: '):
            current_file = line.replace('File: ', '').strip()
            include_section = current_file in source_files
            if include_section:
                filtered_lines.append(line)
        elif line.startswith('-' * 80):
            if include_section:
                filtered_lines.append(line)
        elif include_section:
            filtered_lines.append(line)
    
    file_specific_diff = '\n'.join(filtered_lines)
    print(f"   📊 Filtered diff: {len(file_specific_diff)} chars (from {len(pr_diff)} chars)")
    
    return file_specific_diff if file_specific_diff.strip() else pr_diff

def extract_file_diff_from_pr(pr_diff, source_file_path):
    """Extract diff content for a specific source file from the complete PR diff"""
    if not pr_diff:
        return ""
    
    filtered_lines = []
    current_file = None
    include_section = False
    
    for line in pr_diff.split('\n'):
        if line.startswith('File: '):
            current_file = line.replace('File: ', '').strip()
            include_section = (current_file == source_file_path)
            if include_section:
                filtered_lines.append(line)
        elif line.startswith('-' * 80):
            if include_section:
                filtered_lines.append(line)
                include_section = False  # End of this file's section
        elif include_section:
            filtered_lines.append(line)
    
    return '\n'.join(filtered_lines)

def determine_file_processing_type(source_file_path, file_sections, special_files=None):
    """Determine how to process a file based on operation type and file characteristics"""
    
    # Check if this is a special file (like TOC.md)
    if special_files and os.path.basename(source_file_path) in special_files:
        return "special_file_toc"
    
    # For all other modified files, use regular processing
    return "regular_modified"

def process_regular_modified_file(source_file_path, file_sections, file_diff, pr_url, github_client, ai_client, repo_config, max_sections):
    """Process a regular markdown file that has been modified"""
    try:
        print(f"   📝 Processing as regular modified file: {source_file_path}")
        
        # Extract the actual sections from the file_sections structure
        # file_sections contains: {'sections': {...}, 'original_hierarchy': {...}, 'current_hierarchy': {...}}
        if isinstance(file_sections, dict) and 'sections' in file_sections:
            actual_sections = file_sections['sections']
        else:
            # Fallback: assume file_sections is already the sections dict
            actual_sections = file_sections
        
        print(f"   📊 Extracted sections: {len(actual_sections)} sections")
        
        # CRITICAL: Load the source-diff-dict.json and perform matching
        import json
        import os
        from section_matcher import match_source_diff_to_target
        from pr_analyzer import get_target_hierarchy_and_content
        
        # Load source-diff-dict.json with file prefix
        temp_dir = ensure_temp_output_dir()
        file_prefix = source_file_path.replace('/', '-').replace('.md', '')
        source_diff_dict_file = os.path.join(temp_dir, f"{file_prefix}-source-diff-dict.json")
        if os.path.exists(source_diff_dict_file):
            with open(source_diff_dict_file, 'r', encoding='utf-8') as f:
                source_diff_dict = json.load(f)
            print(f"   📂 Loaded source diff dict with {len(source_diff_dict)} sections from {source_diff_dict_file}")
            
            # Check source token limit before proceeding with processing
            print(f"   🔍 Checking source token limit...")
            within_limit, total_tokens, token_limit = check_source_token_limit(source_diff_dict_file)
            if not within_limit:
                print(f"   🚫 Skipping file processing: source content exceeds token limit")
                print(f"      📊 Total tokens: {total_tokens:,} > Limit: {token_limit:,}")
                print(f"      ⏭️  File {source_file_path} will not be processed")
                return False
                
        else:
            print(f"   ❌ {source_diff_dict_file} not found")
            return False
        
        # Get target file hierarchy and content
        target_repo = repo_config['target_repo']
        target_hierarchy, target_lines = get_target_hierarchy_and_content(source_file_path, github_client, target_repo)
        
        if not target_hierarchy or not target_lines:
            print(f"   ❌ Could not get target file content for {source_file_path}")
            return False
        
        print(f"   📖 Target file: {len(target_hierarchy)} sections, {len(target_lines)} lines")
        
        # Perform source diff to target matching
        print(f"   🔗 Matching source diff to target...")
        enhanced_sections = match_source_diff_to_target(
            source_diff_dict, 
            target_hierarchy, 
            target_lines, 
            ai_client, 
            repo_config, 
            max_sections,
            AI_MAX_TOKENS
        )
        
        if not enhanced_sections:
            print(f"   ❌ No sections matched")
            return False
        
        print(f"   ✅ Matched {len(enhanced_sections)} sections")
        
        # Save the match result for reference
        match_file = os.path.join(temp_dir, f"{source_file_path.replace('/', '-').replace('.md', '')}-match_source_diff_to_target.json")
        with open(match_file, 'w', encoding='utf-8') as f:
            json.dump(enhanced_sections, f, ensure_ascii=False, indent=2)
        print(f"   💾 Saved match result to: {match_file}")
        
        # Step 2: Get AI translation for the matched sections
        print(f"   🤖 Getting AI translation for matched sections...")
        
        # Create file data structure with enhanced matching info
        # Wrap enhanced_sections in the expected format for process_single_file
        file_data = {
            source_file_path: {
                'type': 'enhanced_sections',
                'sections': enhanced_sections
            }
        }
        
        # Call the existing process_modified_sections function to get AI translation
        results = process_modified_sections(file_data, file_diff, pr_url, github_client, ai_client, repo_config, max_sections)
        
        # Step 3: Update match_source_diff_to_target.json with AI results
        if results and len(results) > 0:
            file_path, success, ai_updated_sections = results[0]  # Get first result
            if success and isinstance(ai_updated_sections, dict):
                print(f"   📝 Step 3: Updating {match_file} with AI results...")
                
                # Load current match_source_diff_to_target.json
                with open(match_file, 'r', encoding='utf-8') as f:
                    match_data = json.load(f)
                
                # Add target_new_content field to each section based on AI results
                updated_count = 0
                for key, section_data in match_data.items():
                    operation = section_data.get('source_operation', '')
                    
                    if operation == 'deleted':
                        # For deleted sections, set target_new_content to null
                        section_data['target_new_content'] = None
                    elif key in ai_updated_sections:
                        # For modified/added sections with AI translation
                        section_data['target_new_content'] = ai_updated_sections[key]
                        updated_count += 1
                    else:
                        # For sections not translated, keep original content
                        section_data['target_new_content'] = section_data.get('target_content', '')
                
                # Save updated match_source_diff_to_target.json
                with open(match_file, 'w', encoding='utf-8') as f:
                    json.dump(match_data, f, ensure_ascii=False, indent=2)
                
                print(f"   ✅ Updated {updated_count} sections with AI translations in {match_file}")
                
                # Step 4: Apply updates to target document using update_target_document_from_match_data
                print(f"   📝 Step 4: Applying updates to target document...")
                from file_updater import update_target_document_from_match_data
                
                success = update_target_document_from_match_data(match_file, repo_config['target_local_path'], source_file_path)
                if success:
                    print(f"   🎉 Target document successfully updated!")
                    return True
                else:
                    print(f"   ❌ Failed to update target document")
                    return False
                    
            else:
                print(f"   ⚠️  AI translation failed or returned invalid results")
                return False
        else:
            print(f"   ⚠️  No results from process_modified_sections")
            return False
        
    except Exception as e:
        print(f"   ❌ Error processing regular modified file {source_file_path}: {e}")
        return False


def get_workflow_repo_config(pr_url, repo_configs):
    """Get repository configuration for workflow environment"""
    from pr_analyzer import parse_pr_url
    
    owner, repo, pr_number = parse_pr_url(pr_url)
    source_repo = f"{owner}/{repo}"
    
    if source_repo not in repo_configs:
        raise ValueError(f"Unsupported source repository: {source_repo}. Supported: {list(repo_configs.keys())}")
    
    config = repo_configs[source_repo].copy()
    config['source_repo'] = source_repo
    config['pr_number'] = pr_number
    
    return config

def main():
    """Main function - orchestrates the entire workflow for GitHub Actions"""
    
    # Validate environment variables
    if not all([SOURCE_PR_URL, TARGET_PR_URL, GITHUB_TOKEN, TARGET_REPO_PATH]):
        print("❌ Missing required environment variables:")
        print(f"   SOURCE_PR_URL: {SOURCE_PR_URL}")
        print(f"   TARGET_PR_URL: {TARGET_PR_URL}")
        print(f"   GITHUB_TOKEN: {'Set' if GITHUB_TOKEN else 'Not set'}")
        print(f"   TARGET_REPO_PATH: {TARGET_REPO_PATH}")
        return
    
    print(f"🔧 Auto PR Sync Tool (GitHub Workflow Version)")
    print(f"📍 Source PR URL: {SOURCE_PR_URL}")
    print(f"📍 Target PR URL: {TARGET_PR_URL}")
    print(f"🤖 AI Provider: {AI_PROVIDER}")
    print(f"📁 Target Repo Path: {TARGET_REPO_PATH}")
    
    # Clean and prepare temp_output directory
    clean_temp_output_dir()
    
    # Get repository configuration using workflow config
    try:
        repo_configs = get_workflow_repo_configs()
        repo_config = get_workflow_repo_config(SOURCE_PR_URL, repo_configs)
        print(f"📁 Source Repo: {repo_config['source_repo']} ({repo_config['source_language']})")
        print(f"📁 Target Repo: {repo_config['target_repo']} ({repo_config['target_language']})")
        print(f"📁 Target Path: {repo_config['target_local_path']}")
    except ValueError as e:
        print(f"❌ {e}")
        return
    
    # Initialize clients
    auth = Auth.Token(GITHUB_TOKEN)
    github_client = Github(auth=auth)
    
    # Initialize unified AI client
    try:
        ai_client = UnifiedAIClient(provider=AI_PROVIDER)
        thread_safe_print(f"🤖 AI Provider: {AI_PROVIDER.upper()} ({ai_client.model})")
    except Exception as e:
        thread_safe_print(f"❌ Failed to initialize AI client: {e}")
        return
    
    print(f"\n🚀 Starting auto-sync for PR: {SOURCE_PR_URL}")
    
    # Step 1: Get PR diff
    print(f"\n📋 Step 1: Getting PR diff...")
    pr_diff = get_pr_diff(SOURCE_PR_URL, github_client)
    if not pr_diff:
        print("❌ Could not get PR diff")
        return
    print(f"✅ Got PR diff: {len(pr_diff)} characters")
    
    # Step 2: Analyze source changes with operation categorization
    print(f"\n📊 Step 2: Analyzing source changes...")
    added_sections, modified_sections, deleted_sections, added_files, deleted_files, toc_files = analyze_source_changes(
        SOURCE_PR_URL, github_client, 
        special_files=SPECIAL_FILES, 
        ignore_files=IGNORE_FILES, 
        repo_configs=repo_configs,
        max_non_system_sections=MAX_NON_SYSTEM_SECTIONS_FOR_AI,
        pr_diff=pr_diff  # Pass the PR diff to avoid re-fetching
    )
    
    # Step 3: Process different types of files based on operation type
    print(f"\n📋 Step 3: Processing files based on operation type...")
    
    # Import necessary functions
    from file_updater import process_modified_sections, update_target_document_from_match_data
    from toc_processor import process_toc_files
    
    # Step 3.1: Process deleted files (file-level deletions)
    if deleted_files:
        print(f"\n🗑️  Step 3.1: Processing {len(deleted_files)} deleted files...")
        process_deleted_files(deleted_files, github_client, repo_config)
        print(f"   ✅ Deleted files processed")
    
    # Step 3.2: Process added files (file-level additions)
    if added_files:
        print(f"\n📄 Step 3.2: Processing {len(added_files)} added files...")
        process_added_files(added_files, SOURCE_PR_URL, github_client, ai_client, repo_config)
        print(f"   ✅ Added files processed")
    
    # Step 3.3: Process special files (TOC.md and similar)
    if toc_files:
        print(f"\n📋 Step 3.3: Processing {len(toc_files)} special files (TOC)...")
        process_toc_files(toc_files, SOURCE_PR_URL, github_client, ai_client, repo_config)
        print(f"   ✅ Special files processed")
    
    # Step 3.4: Process modified files (section-level modifications)
    if modified_sections:
        print(f"\n📝 Step 3.4: Processing {len(modified_sections)} modified files...")
        
        # Process each modified file separately
        for source_file_path, file_sections in modified_sections.items():
            print(f"\n📄 Processing modified file: {source_file_path}")
            
            # Extract file-specific diff from the complete PR diff
            print(f"   🔍 Extracting file-specific diff for: {source_file_path}")
            file_specific_diff = extract_file_diff_from_pr(pr_diff, source_file_path)
            
            if not file_specific_diff:
                print(f"   ⚠️  No diff found for {source_file_path}, skipping...")
                continue
            
            print(f"   📊 File-specific diff: {len(file_specific_diff)} chars")
            
            # Determine file processing approach for modified files
            file_type = determine_file_processing_type(source_file_path, file_sections, SPECIAL_FILES)
            print(f"   🔍 File processing type: {file_type}")
            
            if file_type == "special_file_toc":
                # Special files should have been processed in Step 3.3, skip here
                print(f"   ⏭️  Special file already processed in Step 3.3, skipping...")
                continue
            
            elif file_type == "regular_modified":
                # Regular markdown files with modifications
                success = process_regular_modified_file(
                    source_file_path, 
                    file_sections, 
                    file_specific_diff,
                    SOURCE_PR_URL, 
                    github_client, 
                    ai_client, 
                    repo_config, 
                    MAX_NON_SYSTEM_SECTIONS_FOR_AI
                )
                
                if success:
                    print(f"   ✅ Successfully processed {source_file_path}")
                else:
                    print(f"   ❌ Failed to process {source_file_path}")
            
            else:
                print(f"   ⚠️  Unknown file processing type: {file_type} for {source_file_path}, skipping...")
    
    # Final summary
    print(f"📊 Summary:")
    print(f"   📄 Added files: {len(added_files)} processed")
    print(f"   🗑️  Deleted files: {len(deleted_files)} processed")
    print(f"   📋 TOC files: {len(toc_files)} processed")
    print(f"   📝 Modified files: {len(modified_sections)} processed")
    print(f"🎉 Workflow completed successfully!")

if __name__ == "__main__":
    main()
